+++
chapter = false
title = "Lab 3.9 Warm Start"
weight = 12

+++
**Using Warm Start**

Once the previous hyper-parameter tuning job completes, we can perform another round of optimization using `Warm Start`.

Warm start configuration allows you to create a new tuning job with the learning gathered in a parent tuning job by specifying up to 5 parent tuning jobs. If a warm start configuration is specified, Automatic Model Tuning will load the previous \[hyperparameter set, objective metrics values\] to warm start the new tuning job. This means you can continue optimizing your model from the point you finished your previous tuning job experiment.

![](/images/hyper-tuning.png)

In \[ \]:

    import boto3
    import sagemaker
    import pandas as pd
    
    sess = sagemaker.Session()
    bucket = sess.default_bucket()
    role = sagemaker.get_execution_role()
    region = boto3.Session().region_name
    
    sm = boto3.Session().client(service_name="sagemaker", region_name=region)
    

**Pre-Requisite**

**The previous hyper-parameter tuning job needs to complete before we can perform another round of optimization using `Warm Start`.**

In \[ \]:

    %store -r tuning_job_name
    

In \[ \]:

    try:
        tuning_job_name
        print("[OK]")
    except NameError:
        print("+++++++++++++++++++++++++++++++++++++++++++++")
        print("[ERROR] Please run the previous Hyperparameter Tuning notebook.")
        print("+++++++++++++++++++++++++++++++++++++++++++++")
    

In \[ \]:

    print(tuning_job_name)
    

**Check the status of the previous Hyperparameter Job**

In \[ \]:

    job_description = sm.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)
    

In \[ \]:

    if not bool(job_description):
        print("+++++++++++++++++++++++++++++++++++++++++++++")
        print("[ERROR] Please run the previous Hyperparameter Tuning notebook before you continue.")
        print("+++++++++++++++++++++++++++++++++++++++++++++")
    elif job_description["HyperParameterTuningJobStatus"] == "Completed":
        print("[OK] Previous Tuning Job has completed. Please continue.")
    else:
        print("+++++++++++++++++++++++++++++++++++++++++++++")
        print("[ERROR] Please run the previous Hyperparameter Tuning notebook.")
        print("+++++++++++++++++++++++++++++++++++++++++++++")
    

**Specify the S3 Location of the Features**

In \[ \]:

    %store -r processed_train_data_s3_uri
    

In \[ \]:

    try:
        processed_train_data_s3_uri
        print("[OK]")
    except NameError:
        print("+++++++++++++++++++++++++++++++")
        print("[ERROR] Please run the notebooks in the previous PREPARE section before you continue.")
        print("+++++++++++++++++++++++++++++++")
    

In \[ \]:

    print(processed_train_data_s3_uri)
    

In \[ \]:

    %store -r processed_validation_data_s3_uri
    

In \[ \]:

    try:
        processed_validation_data_s3_uri
        print("[OK]")
    except NameError:
        print("+++++++++++++++++++++++++++++++")
        print("[ERROR] Please run the notebooks in the previous PREPARE section before you continue.")
        print("+++++++++++++++++++++++++++++++")
    

In \[ \]:

    print(processed_validation_data_s3_uri)
    

In \[ \]:

    %store -r processed_test_data_s3_uri
    

In \[ \]:

    try:
        processed_test_data_s3_uri
        print("[OK]")
    except NameError:
        print("+++++++++++++++++++++++++++++++")
        print("[ERROR] Please run the notebooks in the previous PREPARE section before you continue.")
        print("+++++++++++++++++++++++++++++++")
    

In \[ \]:

    print(processed_test_data_s3_uri)
    

In \[ \]:

    print(processed_train_data_s3_uri)
    !aws s3 ls $processed_train_data_s3_uri/
    

In \[ \]:

    print(processed_validation_data_s3_uri)
    !aws s3 ls $processed_validation_data_s3_uri/
    

In \[ \]:

    print(processed_test_data_s3_uri)
    !aws s3 ls $processed_test_data_s3_uri/
    

In \[ \]:

    from sagemaker.inputs import TrainingInput
    
    s3_input_train_data = TrainingInput(s3_data=processed_train_data_s3_uri, distribution="ShardedByS3Key")
    s3_input_validation_data = TrainingInput(s3_data=processed_validation_data_s3_uri, distribution="ShardedByS3Key")
    s3_input_test_data = TrainingInput(s3_data=processed_test_data_s3_uri, distribution="ShardedByS3Key")
    
    print(s3_input_train_data.config)
    print(s3_input_validation_data.config)
    print(s3_input_test_data.config)
    

In \[ \]:

    !cat src/tf_bert_reviews.py
    

**Setup Hyper-Parameters for Classification Layer**

First, retrieve `max_seq_length` from the prepare phase.

In \[ \]:

    %store -r max_seq_length
    

In \[ \]:

    try:
        max_seq_length
        print("[OK]")
    except NameError:
        print("+++++++++++++++++++++++++++++++")
        print("[ERROR] Please run the notebooks in the previous PREPARE section before you continue.")
        print("+++++++++++++++++++++++++++++++")
    

In \[ \]:

    print(max_seq_length)
    

In \[ \]:

    epochs = 3
    epsilon = 0.00000001
    train_batch_size = 128
    validation_batch_size = 128
    test_batch_size = 128
    train_steps_per_epoch = 100
    validation_steps = 100
    test_steps = 100
    train_instance_count = 1
    train_instance_type = "ml.c5.4xlarge"
    train_volume_size = 1024
    use_xla = True
    use_amp = True
    enable_sagemaker_debugger = False
    enable_checkpointing = False
    enable_tensorboard = False
    input_mode = "File"
    run_validation = True
    run_test = True
    run_sample_predictions = True
    

**Track the Optimizations Within our Experiment**

In \[ \]:

    %store -r experiment_name
    

In \[ \]:

    try:
        experiment_name
        print("[OK]")
    except NameError:
        print("+++++++++++++++++++++++++++++++")
        print("[ERROR] Please run the notebooks in the previous TRAIN section before you continue.")
        print("+++++++++++++++++++++++++++++++")
    

In \[ \]:

    print(experiment_name)
    

In \[ \]:

    %store -r trial_name
    

In \[ \]:

    try:
        trial_name
        print("[OK]")
    except NameError:
        print("+++++++++++++++++++++++++++++++")
        print("[ERROR] Please run the notebooks in the previous TRAIN section before you continue.")
        print("+++++++++++++++++++++++++++++++")
    

In \[ \]:

    print(trial_name)
    

In \[ \]:

    import time
    from smexperiments.trial import Trial
    
    timestamp = "{}".format(int(time.time()))
    
    trial = Trial.load(trial_name=trial_name)
    print(trial)
    

In \[ \]:

    from smexperiments.tracker import Tracker
    
    tracker_optimize = Tracker.create(display_name="optimize-2", sagemaker_boto_client=sm)
    
    optimize_trial_component_name = tracker_optimize.trial_component.trial_component_name
    print("Optimize trial component name {}".format(optimize_trial_component_name))
    

**Attach the `deploy` Trial Component and Tracker as a Component to the Trial**

In \[ \]:

    trial.add_trial_component(tracker_optimize.trial_component)
    

**Setup Dynamic Hyper-Parameter Ranges to Explore**

While not necessary, we can choose to statically define any hyper-parameters that we are not choosing to explore in this WarmStart optimization run.

In \[ \]:

    from sagemaker.tuner import IntegerParameter
    from sagemaker.tuner import ContinuousParameter
    from sagemaker.tuner import CategoricalParameter
    from sagemaker.tuner import HyperparameterTuner
    
    hyperparameter_ranges = {
        "learning_rate": ContinuousParameter(0.00015, 0.00075, scaling_type="Linear"),
        "train_batch_size": CategoricalParameter([64, 128]),
        "freeze_bert_layer": CategoricalParameter([True, False]),
    }
    

**Track the Hyper-Parameter Ranges**

In \[ \]:

    tracker_optimize.log_parameters(hyperparameter_ranges)
    
    # must save after logging
    tracker_optimize.trial_component.save()
    

**Setup Metrics**

In \[ \]:

    metrics_definitions = [
        {"Name": "train:loss", "Regex": "loss: ([0-9\\.]+)"},
        {"Name": "train:accuracy", "Regex": "accuracy: ([0-9\\.]+)"},
        {"Name": "validation:loss", "Regex": "val_loss: ([0-9\\.]+)"},
        {"Name": "validation:accuracy", "Regex": "val_accuracy: ([0-9\\.]+)"},
    ]
    

In \[ \]:

    from sagemaker.tensorflow import TensorFlow
    
    estimator = TensorFlow(
        entry_point="tf_bert_reviews.py",
        source_dir="src",
        role=role,
        instance_count=train_instance_count,  # Make sure you have at least this number of input files or the ShardedByS3Key distibution strategy will fail the job due to no data available
        instance_type=train_instance_type,
        volume_size=train_volume_size,
        py_version="py37",
        framework_version="2.3.1",
        hyperparameters={
            "epochs": epochs,
            "epsilon": epsilon,
            "validation_batch_size": validation_batch_size,
            "test_batch_size": test_batch_size,
            "train_steps_per_epoch": train_steps_per_epoch,
            "validation_steps": validation_steps,
            "test_steps": test_steps,
            "use_xla": use_xla,
            "use_amp": use_amp,
            "max_seq_length": max_seq_length,
            "enable_sagemaker_debugger": enable_sagemaker_debugger,
            "enable_checkpointing": enable_checkpointing,
            "enable_tensorboard": enable_tensorboard,
            "run_validation": run_validation,
            "run_test": run_test,
            "run_sample_predictions": run_sample_predictions,
        },
        input_mode=input_mode,
        metric_definitions=metrics_definitions,
        #                       max_run=7200 # max 2 hours * 60 minutes seconds per hour * 60 seconds per minute
    )
    

**Setup Warm Start Config**

We configure `WarmStartConfig` using 1 or more of the previous hyper-parameter tuning job runs called the `parent` jobs - as well as a `WarmStartType`. The parents must have finished either with one of the following success or failure states: `Completed`, `Stopped`, or `Failed`.

`WarmStartType` is one of the following strategies:

* `IDENTICAL_DATA_AND_ALGORITHM` uses the same input data and algorithm as the parent tuning jobs, but allows a practitioner to explore more hyper-parameter range values. Upon completion, a tuning job with this strategy will return an additional field, `OverallBestTrainingJob` containing the best model candidate including this tuning job as well as the completed parent tuning jobs.
* `TRANSFER_LEARNING` allows you to transfer the knowledge from previous tuning jobs. You can use different input datasets and algorithms - as well as everything from the `IDENTICAL_DATA_AND_ALGORITHM` strategy.

_Note: Recursive parent-child relationships are not supported._

In \[ \]:

    print("Previous Tuning Job Name: {}".format(tuning_job_name))
    

In \[ \]:

    from sagemaker.tuner import WarmStartConfig
    from sagemaker.tuner import WarmStartTypes
    
    warm_start_config = WarmStartConfig(
        warm_start_type=WarmStartTypes.IDENTICAL_DATA_AND_ALGORITHM, parents={tuning_job_name}
    )
    

**Setup HyperparameterTuner with Warm Start Config including New Hyper-Parameter Ranges**

In \[ \]:

    objective_metric_name = "train:accuracy"
    
    tuner = HyperparameterTuner(
        estimator=estimator,
        objective_type="Maximize",
        objective_metric_name=objective_metric_name,
        hyperparameter_ranges=hyperparameter_ranges,
        metric_definitions=metrics_definitions,
        max_jobs=2,
        max_parallel_jobs=1,
        strategy="Bayesian",
        early_stopping_type="Auto",
        warm_start_config=warm_start_config,
    )
    

**Start Tuning Job**

In \[ \]:

    tuner.fit(
        {"train": s3_input_train_data, "validation": s3_input_validation_data, "test": s3_input_test_data},
        include_cls_metadata=False,
        wait=False,
    )
    

**If You See an Error, Please Wait for the Hyper-Parameter Tuning Job to Complete from the Previous Notebook**

**Check Tuning Job Status**

Re-run this cell to track the status.

In \[ \]:

    from pprint import pprint
    
    tuning_job_name = tuner.latest_tuning_job.job_name
    
    job_description = sm.describe_hyper_parameter_tuning_job(HyperParameterTuningJobName=tuning_job_name)
    
    status = job_description["HyperParameterTuningJobStatus"]
    
    print("\n")
    print(status)
    print("\n")
    pprint(job_description)
    
    if status != "Completed":
        job_count = job_description["TrainingJobStatusCounters"]["Completed"]
        print("Not yet complete, but {} jobs have completed.".format(job_count))
    
        if job_description.get("BestTrainingJob", None):
            print("Best candidate:")
            pprint(job_description["BestTrainingJob"]["TrainingJobName"])
            pprint(job_description["BestTrainingJob"]["FinalHyperParameterTuningJobObjectiveMetric"])
        else:
            print("No training jobs have reported results yet.")
    

In \[ \]:

    from IPython.core.display import display, HTML
    
    display(
        HTML(
            'Review {}#/hyper-tuning-jobs/{}">Hyper-Parameter Tuning Job'.format(
                region, tuning_job_name
            )
        )
    )
    

_Please Wait for the ^^ Tuning Job ^^ to Complete Above_

In \[ \]:

    tuner.wait()
    

**Show the Tuning Job**

> _Note: This will fail at first. Please wait about 15-30 seconds and re-run._

In \[ \]:

    from sagemaker.analytics import HyperparameterTuningJobAnalytics
    
    hp_results = HyperparameterTuningJobAnalytics(sagemaker_session=sess, hyperparameter_tuning_job_name=tuning_job_name)
    
    df_results = hp_results.dataframe()
    df_results.shape
    

In \[ \]:

    df_results.sort_values("FinalObjectiveValue", ascending=0)
    

**Show the Overall Best Candidate**

In \[ \]:

    df_results.sort_values("FinalObjectiveValue", ascending=0).head(1)
    

In \[ \]:

    best_candidate_tuning_job_name = df_results.sort_values("FinalObjectiveValue", ascending=0).head(1)["TrainingJobName"]
    

**Log the Best Hyper-Parameter and Objective Metric in the Experiment**

Logging `learning_rate` parameter and `accuracy` metric

In \[ \]:

    best_learning_rate = df_results.sort_values("FinalObjectiveValue", ascending=0).head(1)["learning_rate"]
    print(best_learning_rate)
    

In \[ \]:

    best_accuracy = df_results.sort_values("FinalObjectiveValue", ascending=0).head(1)["FinalObjectiveValue"]
    print(best_accuracy)
    

In \[ \]:

    tracker_optimize.log_parameters({"learning_rate": float(best_learning_rate)})
    
    # must save after logging
    tracker_optimize.trial_component.save()
    

In \[ \]:

    tracker_optimize.log_metric("accuracy", float(best_accuracy))
    
    tracker_optimize.trial_component.save()
    

_Ignore any ^^ ERROR above ^^. This is OK._

**Show Experiment Analytics**

In \[ \]:

    from sagemaker.analytics import ExperimentAnalytics
    
    lineage_table = ExperimentAnalytics(
        sagemaker_session=sess,
        experiment_name=experiment_name,
        metric_names=["validation:accuracy"],
        sort_by="CreationTime",
        sort_order="Descending",
    )
    
    lineage_df = lineage_table.dataframe()
    lineage_df.shape
    

In \[ \]:

    lineage_df
    

**Pass `tuning_job_name` to the Next Notebook**

In \[ \]:

    print(best_candidate_tuning_job_name)
    

In \[ \]:

    %store best_candidate_tuning_job_name
    

In \[ \]:

    %store
    

**Release Resources**

In \[ \]:

    %%html
    
    <p><b>Shutting down your kernel for this notebook to release resources.b>p>
    <button class="sm-command-button" data-commandlinker-command="kernelmenu:shutdown" style="display:none;">Shutdown Kernelbutton>
            
    <script>
    try {
        els = document.getElementsByClassName("sm-command-button");
        els[0].click();
    }
    catch(err) {
        // NoOp
    }    
    script>
    

In \[ \]:

    %%javascript
    
    try {
        Jupyter.notebook.save_checkpoint();
        Jupyter.notebook.session.delete();
    }
    catch(err) {
        // NoOp
    }