+++
chapter = false
title = "Lab 2.1.5 Query with Wrangler"
weight = 6

+++
**AWS Data Wrangler** is an open-source Python library that extends the power of the Pandas library to AWS connecting DataFrames and AWS data-related services (Amazon Redshift, AWS Glue, Amazon Athena, Amazon EMR, Amazon QuickSight, etc).

* [https://github.com/awslabs/aws-data-wrangler](https://github.com/awslabs/aws-data-wrangler "https://github.com/awslabs/aws-data-wrangler")
* [https://aws-data-wrangler.readthedocs.io](https://aws-data-wrangler.readthedocs.io "https://aws-data-wrangler.readthedocs.io")

Built on top of other open-source projects like Pandas, Apache Arrow, Boto3, s3fs, SQLAlchemy, Psycopg2 and PyMySQL, it offers abstracted functions to execute usual ETL tasks like load/unload data from Data Lakes, Data Warehouses and Databases.

_Note that AWS Data Wrangler is simply a Python library that uses existing AWS Services. AWS Data Wrangler is not a separate AWS Service. You install AWS Data Wrangler through `pip install` as we will see next._

_Pre-Requisite: Make Sure You Created an Athena Table for Both TSV and Parquet in Previous Notebooks_

In \[ \]:

    %store -r ingest_create_athena_table_tsv_passed

In \[ \]:

    try:
        ingest_create_athena_table_tsv_passed
    except NameError:
        print("++++++++++++++++++++++++++++++++++++++++++++++")
        print("[ERROR] YOU HAVE TO RUN ALL PREVIOUS NOTEBOOKS.  You did not register the TSV Data.")
        print("++++++++++++++++++++++++++++++++++++++++++++++")

In \[ \]:

    print(ingest_create_athena_table_tsv_passed)

In \[ \]:

    if not ingest_create_athena_table_tsv_passed:
        print("++++++++++++++++++++++++++++++++++++++++++++++")
        print("[ERROR] YOU HAVE TO RUN ALL PREVIOUS NOTEBOOKS.  You did not register the TSV Data.")
        print("++++++++++++++++++++++++++++++++++++++++++++++")
    else:
        print("[OK]")

In \[ \]:

    %store -r ingest_create_athena_table_parquet_passed

In \[ \]:

    try:
        ingest_create_athena_table_parquet_passed
    except NameError:
        print("++++++++++++++++++++++++++++++++++++++++++++++")
        print("[ERROR] YOU HAVE TO RUN ALL PREVIOUS NOTEBOOKS.  You did not convert into Parquet data.")
        print("++++++++++++++++++++++++++++++++++++++++++++++")

In \[ \]:

    print(ingest_create_athena_table_parquet_passed)

In \[ \]:

    if not ingest_create_athena_table_parquet_passed:
        print("++++++++++++++++++++++++++++++++++++++++++++++")
        print("[ERROR] YOU HAVE TO RUN ALL PREVIOUS NOTEBOOKS.  You did not convert into Parquet data.")
        print("++++++++++++++++++++++++++++++++++++++++++++++")
    else:
        print("[OK]")

**Setup**

In \[ \]:

    import sagemaker
    import boto3
    
    sess = sagemaker.Session()
    bucket = sess.default_bucket()
    role = sagemaker.get_execution_role()
    region = boto3.Session().region_name
    
    sm = boto3.Session().client(service_name="sagemaker", region_name=region)

In \[ \]:

    import awswrangler as wr

**Query Parquet from S3 with Push-Down Filters**

Read Apache Parquet file(s) from a received S3 prefix or list of S3 object paths.

The concept of Dataset goes beyond the simple idea of files and enables more complex features like partitioning and catalogue integration (AWS Glue Catalog):

_dataset (bool)_ â€“ If True read a parquet dataset instead of simple file(s) loading all the related partitions as columns.

In \[ \]:

    p_filter = lambda x: x["product_category"] == "Digital_Software"

In \[ \]:

    path = "s3://{}/amazon-reviews-pds/parquet/".format(bucket)
    df_parquet_results = wr.s3.read_parquet(
        path, columns=["star_rating", "product_category", "review_body"], partition_filter=p_filter, dataset=True
    )
    df_parquet_results.shape

In \[ \]:

    df_parquet_results.head(5)

**Query Parquet from S3 in Chunks**

Batching (chunked argument) (Memory Friendly):

This will enable the function to return an Iterable of DataFrames instead of a regular DataFrame.

There are two batching strategies on Wrangler:

* If chunked=True, a new DataFrame will be returned for each file in your path/dataset.
* If chunked=INTEGER, Wrangler will iterate on the data by several rows equal to the received INTEGER.

P.S. chunked=True if faster and uses less memory while chunked=INTEGER is more precise in the number of rows for each Dataframe.

In \[ \]:

    path = "s3://{}/amazon-reviews-pds/parquet/".format(bucket)
    chunk_iter = wr.s3.read_parquet(
        path,
        columns=["star_rating", "product_category", "review_body"],
        # filters=[("product_category", "=", "Digital_Software")],
        partition_filter=p_filter,
        dataset=True,
        chunked=True,
    )

In \[ \]:

    print(next(chunk_iter))

**Query the Glue Catalog (ie. Hive Metastore)**

Get an iterator of tables.

In \[ \]:

    database_name = "dsoaws"
    table_name_tsv = "amazon_reviews_tsv"
    table_name_parquet = "amazon_reviews_parquet"

In \[ \]:

    for table in wr.catalog.get_tables(database="dsoaws"):
        print(table["Name"])

**Query from Athena**

Execute any SQL query on AWS Athena and return the results as a Pandas DataFrame.

In \[ \]:

    %%time
    df = wr.athena.read_sql_query(sql="SELECT * FROM {} LIMIT 5000".format(table_name_parquet), database=database_name)

In \[ \]:

    df.head(5)

**Query from Athena in Chunks**

Retrieving in chunks can help reduce memory requirements.

_This will take a few seconds._

In \[ \]:

    %%time
    
    chunk_iter = wr.athena.read_sql_query(
        sql="SELECT * FROM {} LIMIT 5000".format(table_name_parquet),
        database="{}".format(database_name),
        chunksize=64_000,  # 64 KB Chunks
    )

In \[ \]:

    print(next(chunk_iter))

**Release Resources**

In \[ \]:

    %%html
    
    <p><b>Shutting down your kernel for this notebook to release resources.b>p>
    <button class="sm-command-button" data-commandlinker-command="kernelmenu:shutdown" style="display:none;">Shutdown Kernelbutton>
            
    <script>
    try {
        els = document.getElementsByClassName("sm-command-button");
        els[0].click();
    }
    catch(err) {
        // NoOp
    }    
    script>

In \[ \]:

    %%javascript
    
    try {
        Jupyter.notebook.save_checkpoint();
        Jupyter.notebook.session.delete();
    }
    catch(err) {
        // NoOp
    }